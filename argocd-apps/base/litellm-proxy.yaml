apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: litellm-proxy
  namespace: argocd
spec:
  project: default
  source:
    chart: litellm-helm
    repoURL: ghcr.io/berriai
    targetRevision: 0.1.819@sha256:ddec70e5365eadea8753a015bf953c01d8eb28ef787aad34004262c32e466f8b
    helm:
      valuesObject:
        proxy_config:
          general_settings:
            store_model_in_db: true
            store_prompts_in_spend_logs: true
          model_list:
            - model_name: "gemma3:27b"
              litellm_params:
                model: "ollama/gemma3:27b-it-qat"
                api_base: "http://r2d2.echobase.network:11434"
            - model_name: "llama3.2"
              litellm_params:
                model: "ollama/llama3.2:latest"
                api_base: "http://r2d2.echobase.network:11434"
        service:
          port: 6000
          type: LoadBalancer
        ingress:
          enabled: true
          hosts:
            - host: llmproxy.echobase.network
              paths:
                - path: /
                  pathType: ImplementationSpecific
      releaseName: litellm-proxy
  destination:
    server: https://kubernetes.default.svc
    namespace: litellm
  syncPolicy:
    syncOptions:
      - CreateNamespace=true
    automated:
      prune: true
      selfHeal: true
