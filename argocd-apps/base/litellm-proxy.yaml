apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: litellm-proxy
  namespace: argocd
spec:
  project: default
  source:
    chart: litellm-helm
    repoURL: ghcr.io/berriai
    targetRevision: 0.1.819
    helm:
      valuesObject:
        proxy_config:
          general_settings:
            store_model_in_db: true
            store_prompts_in_spend_logs: true
          model_list:
            - model_name: "gemma3:27b"
              litellm_params:
                model: "ollama/gemma3:27b-it-qat"
                api_base: "http://r2d2.echobase.network:11434"
            - model_name: "llama3.2"
              litellm_params:
                model: "ollama/llama3.2:latest"
                api_base: "http://r2d2.echobase.network:11434"
            - model_name: "claude sonnet"
              litellm_params:
                model: "anthropic/claude-sonnet-4-5-20250929"
        service:
          port: 6000
          type: LoadBalancer
        ingress:
          enabled: true
          annotations:
            cert-manager.io/cluster-issuer: letsencrypt-prod
            nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
          hosts:
            - host: llmproxy.echobase.network
              paths:
                - path: /
                  pathType: ImplementationSpecific
          tls:
            - secretName: llmproxy-prod-tls
              hosts:
                - llmproxy.echobase.network
      releaseName: litellm-proxy
  destination:
    server: https://kubernetes.default.svc
    namespace: litellm
  syncPolicy:
    syncOptions:
      - CreateNamespace=true
    automated:
      prune: true
      selfHeal: true
